# Copyright Â© Advanced Micro Devices, Inc., or its affiliates.
#
# SPDX-License-Identifier: MIT

"""Tests for the CommandGenerator class."""

import os
import stat
import sys
import tempfile
from enum import Enum
from unittest.mock import Mock, patch

import pytest

# TODO: Remove this compatibility workaround once the ROCm base image is updated to Python 3.12
if sys.version_info >= (3, 11):
    from enum import StrEnum
else:

    class StrEnum(str, Enum):
        """Minimal StrEnum for Python <3.11."""


from aim_common import Engine, GPUModel, Metric, Precision
from aim_runtime import ProfileRegistry
from aim_runtime.command_generator import CommandGenerator
from aim_runtime.config import AIMConfig
from aim_runtime.model_cache_resolver import ModelCacheResolver
from aim_runtime.object_model import Profile
from aim_runtime.profile_validator import ProfileValidator


@pytest.fixture
def command_generator(aim_config: AIMConfig) -> CommandGenerator:
    """Create a CommandGenerator instance."""
    return CommandGenerator(aim_config)


@pytest.fixture
def command_generator_model_outside_profiles_path(
    general_aim_config: AIMConfig,
) -> CommandGenerator:
    """Create a CommandGenerator instance without a model."""
    return CommandGenerator(general_aim_config)


@pytest.fixture
def command_generator_no_model(faulty_aim_config_with_no_model: AIMConfig) -> CommandGenerator:
    """Create a CommandGenerator instance without a model."""
    return CommandGenerator(faulty_aim_config_with_no_model)


@pytest.fixture
def minimal_profile(profiles_path, profile_validator: ProfileValidator) -> Profile:
    """Create a minimal profile for testing."""
    registry = ProfileRegistry.discover_and_validate(search_paths=[profiles_path], validator=profile_validator)
    return registry.find_by_id("minimal_profile")


@pytest.fixture
def minimal_profile_no_model(general_profiles_path, profile_validator: ProfileValidator) -> Profile:
    """Create a minimal profile for testing."""
    registry = ProfileRegistry.discover_and_validate(search_paths=[general_profiles_path], validator=profile_validator)
    return registry.find_by_id("general/minimal_profile_no_model")


def test_command_generator_initialization(command_generator: CommandGenerator, aim_config: AIMConfig) -> None:
    """Test that CommandGenerator initializes correctly."""
    assert command_generator.config == aim_config


def test_generate_command_script_success(command_generator: CommandGenerator, model_profile: Profile) -> None:
    """Test successful command script generation."""
    script_path = command_generator.generate_command_script(model_profile)

    # Verify script file exists
    assert os.path.exists(script_path)
    assert script_path.endswith(".sh")

    # Verify script is executable
    st = os.stat(script_path)
    assert st.st_mode & stat.S_IEXEC

    # Verify script content
    with open(script_path, "r") as f:
        content = f.read()

    assert "#!/bin/bash" in content
    assert "set -e" in content
    assert "Generated by AIM Command Generator" in content
    assert "export VLLM_DO_NOT_TRACK=" in content
    assert "exec python" in content
    assert "vllm.entrypoints.openai.api_server" in content
    assert "--model meta-llama/Llama-3.1-8B-Instruct" in content

    # Clean up
    os.unlink(script_path)


def test_generate_command_script_general_profile_success(
    command_generator_model_outside_profiles_path: CommandGenerator, general_profile: Profile
) -> None:
    """Test successful command script generation."""
    script_path = command_generator_model_outside_profiles_path.generate_command_script(general_profile)

    # Verify script file exists
    assert os.path.exists(script_path)
    assert script_path.endswith(".sh")

    # Verify script is executable
    st = os.stat(script_path)
    assert st.st_mode & stat.S_IEXEC

    # Verify script content
    with open(script_path, "r") as f:
        content = f.read()

    assert "#!/bin/bash" in content
    assert "set -e" in content
    assert "Generated by AIM Command Generator" in content
    assert "export VLLM_DO_NOT_TRACK=" in content
    assert "exec python" in content
    assert "vllm.entrypoints.openai.api_server" in content
    assert "--model meta-llama/Llama-3.1-8B-Instruct" in content

    # Clean up
    os.unlink(script_path)


def test_build_command(command_generator: CommandGenerator, model_profile: Profile) -> None:
    """Test command building from profile."""
    command = command_generator._build_command(model_profile)

    assert isinstance(command, str)
    assert "python" in command
    assert "-m vllm.entrypoints.openai.api_server" in command
    assert "--model meta-llama/Llama-3.1-8B-Instruct" in command
    assert "--dtype float16" in command
    assert "--port 8000" in command
    assert "--enable-eplb" in command


def test_build_command_list_with_model_in_profile(command_generator: CommandGenerator, model_profile: Profile) -> None:
    """Test command list building when model is in profile."""
    command_list = command_generator._build_command_list(model_profile)

    assert command_list[0] in ["python", "python3"]
    assert command_list[1] == "-m"
    assert command_list[2] == "vllm.entrypoints.openai.api_server"
    assert "--model" in command_list
    model_index = command_list.index("--model")
    assert command_list[model_index + 1] == "meta-llama/Llama-3.1-8B-Instruct"


def test_build_command_list_with_quantized_model(
    command_generator: CommandGenerator, profile_validator: ProfileValidator, profiles_path: str
) -> None:
    """Test command list building when model_id is different from aim_id (e.g., quantized model)."""
    # Load the quantized profile
    registry = ProfileRegistry.discover_and_validate(search_paths=[profiles_path], validator=profile_validator)
    quantized_profile = registry.find_by_id("test_profile_quantized")
    command_list = command_generator._build_command_list(quantized_profile)
    assert "--model" in command_list
    model_index = command_list.index("--model")
    # Should use model_id value (the actual model to load)
    assert command_list[model_index + 1] == "amd/Llama-3.1-8B-Instruct-FP8-KV"


def test_build_command_list_with_model_in_config(command_generator: CommandGenerator, minimal_profile: Profile) -> None:
    """Test command list building when model is in config only."""
    command_list = command_generator._build_command_list(minimal_profile)

    assert "--model" in command_list
    model_index = command_list.index("--model")
    assert command_list[model_index + 1] == "test/model"


def test_build_command_list_no_model(
    command_generator_no_model: CommandGenerator, minimal_profile_no_model: Profile
) -> None:
    """Test command list building when no model is specified."""
    with pytest.raises(ValueError, match="Model not specified"):
        command_generator_no_model._build_command_list(minimal_profile_no_model)


def test_get_engine_module_vllm(command_generator: CommandGenerator) -> None:
    """Test engine module resolution for vllm."""
    module = command_generator._get_engine_module(Engine.VLLM)
    assert module == "vllm.entrypoints.openai.api_server"


def test_get_engine_module_unsupported(command_generator: CommandGenerator) -> None:
    """Test engine module resolution for unsupported engine."""

    class DummyEngine(StrEnum):
        UNSUPPORTED = "unsupported"

    with pytest.raises(ValueError, match=r"Unsupported engine: unsupported\. Supported engines: \['auto', 'vllm'\]"):
        command_generator._get_engine_module(DummyEngine.UNSUPPORTED)


def test_build_engine_args_comprehensive(command_generator: CommandGenerator, model_profile: Profile) -> None:
    """Test comprehensive engine args building."""
    engine_args = model_profile.engine_args
    args_list = command_generator._build_engine_args(engine_args)

    # Check various argument types
    assert "--dtype" in args_list
    assert "float16" in args_list

    assert "--tensor-parallel-size" in args_list
    assert "1" in args_list

    # Check null flag (should appear as flag only)
    assert "--no-enable-chunked-prefill" in args_list

    # Check boolean true (should appear as flag only)
    assert "--enable-eplb" in args_list

    # Check boolean false (should not appear)
    # We need to test with explicit false value
    test_args = {"debug": False, "verbose": True}
    result = command_generator._build_engine_args(test_args)
    assert "--debug" not in result
    assert "--verbose" in result


def test_build_engine_args_empty(command_generator: CommandGenerator) -> None:
    """Test engine args building with empty args."""
    args_list = command_generator._build_engine_args({})
    assert args_list == []


def test_build_engine_args_various_types(command_generator: CommandGenerator) -> None:
    """Test engine args building with various value types."""
    test_args = {
        "string-arg": "value",
        "int-arg": 123,
        "float-arg": 3.14,
        "bool-true": True,
        "bool-false": False,
        "null-arg": None,
        "list-arg": ["item1", "item2"],
        "tuple-arg": ("item3", "item4"),
    }

    args_list = command_generator._build_engine_args(test_args)

    # String argument
    assert "--string-arg" in args_list
    assert "value" in args_list

    # Integer argument
    assert "--int-arg" in args_list
    assert "123" in args_list

    # Float argument
    assert "--float-arg" in args_list
    assert "3.14" in args_list

    # Boolean true
    assert "--bool-true" in args_list

    # Boolean false (should not appear)
    assert "--bool-false" not in args_list

    # Null argument (flag only)
    assert "--null-arg" in args_list

    # List/tuple arguments (flag once, followed by all values)
    list_arg_index = args_list.index("--list-arg")
    assert args_list[list_arg_index + 1] == "item1"
    assert args_list[list_arg_index + 2] == "item2"
    # Verify only one occurrence of the flag
    assert args_list.count("--list-arg") == 1

    tuple_arg_index = args_list.index("--tuple-arg")
    assert args_list[tuple_arg_index + 1] == "item3"
    assert args_list[tuple_arg_index + 2] == "item4"
    # Verify only one occurrence of the flag
    assert args_list.count("--tuple-arg") == 1


def test_create_script_content(command_generator: CommandGenerator) -> None:
    """Test script content creation."""
    test_command = "python -m vllm.entrypoints.openai.api_server --model test"
    test_env_vars = {"VAR1": "value1", "VAR2": "value with spaces", "VAR3": 123}

    script_content = command_generator._create_script_content(test_command, test_env_vars)

    assert script_content.startswith("#!/bin/bash")
    assert "set -e" in script_content
    assert "Generated by AIM Command Generator" in script_content
    assert "export VAR1=value1" in script_content
    assert "export VAR2='value with spaces'" in script_content
    assert "export VAR3=123" in script_content
    assert f"exec {test_command}" in script_content


def test_create_script_content_no_env_vars(command_generator: CommandGenerator) -> None:
    """Test script content creation without environment variables."""
    test_command = "python -m vllm.entrypoints.openai.api_server --model test"

    script_content = command_generator._create_script_content(test_command, {})

    assert script_content.startswith("#!/bin/bash")
    assert "set -e" in script_content
    assert "Generated by AIM Command Generator" in script_content
    assert "export" not in script_content
    assert f"exec {test_command}" in script_content


def test_write_script_file(command_generator: CommandGenerator) -> None:
    """Test script file writing."""
    test_content = "#!/bin/bash\necho 'test'"

    script_path = command_generator._write_script_file(test_content)

    try:
        # Verify file exists and has correct content
        assert os.path.exists(script_path)

        with open(script_path, "r") as f:
            content = f.read()
        assert content == test_content

        # Verify file is executable
        st = os.stat(script_path)
        assert st.st_mode & stat.S_IEXEC

        # Verify it's a temporary file
        assert script_path.startswith(tempfile.gettempdir())
        assert "aim-serve-" in script_path
        assert script_path.endswith(".sh")

    finally:
        # Clean up
        if os.path.exists(script_path):
            os.unlink(script_path)


@patch("tempfile.mkstemp")
@patch("os.fdopen")
def test_write_script_file_write_failure(mock_fdopen, mock_mkstemp, command_generator: CommandGenerator) -> None:
    """Test script file writing with write failure."""
    # Mock mkstemp to return a fake file descriptor and path
    fake_fd = 123
    fake_path = "/tmp/fake-script.sh"
    mock_mkstemp.return_value = (fake_fd, fake_path)

    # Mock fdopen to raise an exception when writing
    mock_file = Mock()
    mock_file.write.side_effect = Exception("Write failed")
    mock_fdopen.return_value.__enter__.return_value = mock_file

    # Mock os.unlink to verify cleanup
    with patch("os.unlink") as mock_unlink:
        with pytest.raises(Exception, match="Write failed"):
            command_generator._write_script_file("test content")

        # Verify cleanup was attempted
        mock_unlink.assert_called_once_with(fake_path)


@patch("shutil.which")
def test_build_command_list_python_fallback(
    mock_which, command_generator: CommandGenerator, minimal_profile: Profile
) -> None:
    """Test command list building falls back to python3 when python is not available."""
    # Mock shutil.which to return None for 'python' (not found)
    mock_which.return_value = None

    command_list = command_generator._build_command_list(minimal_profile)

    # Should use python3 when python is not available
    assert command_list[0] == "python3"


@patch("shutil.which")
def test_build_command_list_python_available(
    mock_which, command_generator: CommandGenerator, minimal_profile: Profile
) -> None:
    """Test command list building uses python when available."""
    # Mock shutil.which to return a path for 'python' (found)
    mock_which.return_value = "/usr/bin/python"

    command_list = command_generator._build_command_list(minimal_profile)

    # Should use python when available
    assert command_list[0] == "python"


def test_build_command_list_with_local_dir_cache(
    command_generator: CommandGenerator, model_profile: Profile, tmp_path
) -> None:
    """Test command list building when model is in local directory cache."""
    # Setup local directory cache
    cache_dir = tmp_path / "model-cache"
    local_model_path = cache_dir / "meta-llama" / "Llama-3.1-8B-Instruct"
    local_model_path.mkdir(parents=True, exist_ok=True)

    # Update config to use temp cache dir
    command_generator.config.cache_dir = str(cache_dir)
    # Recreate resolver with new cache dir
    command_generator.cache_resolver = ModelCacheResolver(str(cache_dir))

    command_list = command_generator._build_command_list(model_profile)

    # Should use local path as model
    assert "--model" in command_list
    model_index = command_list.index("--model")
    assert command_list[model_index + 1] == str(local_model_path)

    # Should set served-model-name once with single value (deduplicated since aim_id == model_id)
    # The list is: --served-model-name model_id
    served_model_indices = [i for i, arg in enumerate(command_list) if arg == "--served-model-name"]
    assert len(served_model_indices) == 1, "Expected one --served-model-name flag (deduplicated)"
    assert command_list[served_model_indices[0] + 1] == "meta-llama/Llama-3.1-8B-Instruct"  # model_id
    # Verify only one value follows (next arg should be a different flag or end of list)
    next_index = served_model_indices[0] + 2
    assert next_index >= len(command_list) or command_list[next_index].startswith(
        "--"
    ), "Expected only one model name after --served-model-name"


def test_build_command_list_with_hf_cache(
    command_generator: CommandGenerator, model_profile: Profile, tmp_path
) -> None:
    """Test command list building when using HF Hub cache."""
    # Setup HF Hub cache directory (but no local dir)
    cache_dir = tmp_path / "model-cache"
    hf_hub_dir = cache_dir / "hub"
    hf_hub_dir.mkdir(parents=True, exist_ok=True)

    # Update config to use temp cache dir
    command_generator.config.cache_dir = str(cache_dir)
    # Recreate resolver with new cache dir
    command_generator.cache_resolver = ModelCacheResolver(str(cache_dir))

    command_list = command_generator._build_command_list(model_profile)

    # Should use model_id (HF will resolve from cache)
    assert "--model" in command_list
    model_index = command_list.index("--model")
    assert command_list[model_index + 1] == "meta-llama/Llama-3.1-8B-Instruct"

    # Should set served-model-name once with single value (deduplicated since aim_id == model_id)
    served_model_indices = [i for i, arg in enumerate(command_list) if arg == "--served-model-name"]
    assert len(served_model_indices) == 1, "Expected one --served-model-name flag (deduplicated)"
    assert command_list[served_model_indices[0] + 1] == "meta-llama/Llama-3.1-8B-Instruct"
    # Verify only one value follows (next arg should be a different flag or end of list)
    next_index = served_model_indices[0] + 2
    assert next_index >= len(command_list) or command_list[next_index].startswith(
        "--"
    ), "Expected only one model name after --served-model-name"


def test_build_command_list_no_cache_exists(
    command_generator: CommandGenerator, model_profile: Profile, tmp_path
) -> None:
    """Test command list building when no cache exists (will download from HF)."""
    # Setup empty cache directory
    cache_dir = tmp_path / "model-cache"
    cache_dir.mkdir(parents=True, exist_ok=True)

    # Update config to use temp cache dir
    command_generator.config.cache_dir = str(cache_dir)
    # Recreate resolver with new cache dir
    command_generator.cache_resolver = ModelCacheResolver(str(cache_dir))

    command_list = command_generator._build_command_list(model_profile)

    # Should use model_id (HF will download)
    assert "--model" in command_list
    model_index = command_list.index("--model")
    assert command_list[model_index + 1] == "meta-llama/Llama-3.1-8B-Instruct"

    # Should set served-model-name once with single value (deduplicated since aim_id == model_id)
    served_model_indices = [i for i, arg in enumerate(command_list) if arg == "--served-model-name"]
    assert len(served_model_indices) == 1, "Expected one --served-model-name flag (deduplicated)"
    assert command_list[served_model_indices[0] + 1] == "meta-llama/Llama-3.1-8B-Instruct"
    # Verify only one value follows (next arg should be a different flag or end of list)
    next_index = served_model_indices[0] + 2
    assert next_index >= len(command_list) or command_list[next_index].startswith(
        "--"
    ), "Expected only one model name after --served-model-name"


def test_build_command_list_with_local_dir_cache_no_aim_id(
    command_generator_model_outside_profiles_path: CommandGenerator, model_profile: Profile, tmp_path
) -> None:
    """Test command list building with local dir cache but no AIM ID (base container)."""
    # Setup local directory cache
    cache_dir = tmp_path / "model-cache"
    local_model_path = cache_dir / "meta-llama" / "Llama-3.1-8B-Instruct"
    local_model_path.mkdir(parents=True, exist_ok=True)

    # Update config to use temp cache dir
    command_generator_model_outside_profiles_path.config.cache_dir = str(cache_dir)
    # Recreate resolver with new cache dir
    command_generator_model_outside_profiles_path.cache_resolver = ModelCacheResolver(str(cache_dir))

    command_list = command_generator_model_outside_profiles_path._build_command_list(model_profile)

    # Should use local path as model
    assert "--model" in command_list
    model_index = command_list.index("--model")
    assert command_list[model_index + 1] == str(local_model_path)

    # Should set served-model-name once with single value (no AIM ID)
    served_model_indices = [i for i, arg in enumerate(command_list) if arg == "--served-model-name"]
    assert len(served_model_indices) == 1, "Expected one --served-model-name flag (no AIM ID)"
    assert command_list[served_model_indices[0] + 1] == "meta-llama/Llama-3.1-8B-Instruct"  # model_id only
    # Verify only one value follows (next arg should be a different flag or end of list)
    next_index = served_model_indices[0] + 2
    assert next_index >= len(command_list) or command_list[next_index].startswith(
        "--"
    ), "Expected only one model name after --served-model-name"


def test_build_command_list_with_local_dir_cache_different_aim_id(
    model_profile: Profile, tmp_path, schemas_path: str, profiles_path: str
) -> None:
    """Test command list building with local dir cache where aim_id differs from model_id."""
    # Create config with different aim_id than model_id
    config = AIMConfig(
        aim_id="custom-aim-id",  # Different from model_id
        model_id=None,
        schema_search_path=schemas_path,
        profile_base_path=profiles_path,
        precision=Precision.FP16,
        engine=Engine.VLLM,
        metric=Metric.LATENCY,
        gpu_count="1",
        gpu_model=GPUModel.NONE,
    )
    command_generator = CommandGenerator(config)

    # Setup local directory cache
    cache_dir = tmp_path / "model-cache"
    local_model_path = cache_dir / "meta-llama" / "Llama-3.1-8B-Instruct"
    local_model_path.mkdir(parents=True, exist_ok=True)

    # Update config to use temp cache dir
    command_generator.config.cache_dir = str(cache_dir)
    # Recreate resolver with new cache dir
    command_generator.cache_resolver = ModelCacheResolver(str(cache_dir))

    command_list = command_generator._build_command_list(model_profile)

    # Should use local path as model
    assert "--model" in command_list
    model_index = command_list.index("--model")
    assert command_list[model_index + 1] == str(local_model_path)

    # Should set served-model-name once with two values (model_id first, then aim_id)
    served_model_indices = [i for i, arg in enumerate(command_list) if arg == "--served-model-name"]
    assert len(served_model_indices) == 1, "Expected one --served-model-name flag with two values (different aim_id)"
    assert command_list[served_model_indices[0] + 1] == "meta-llama/Llama-3.1-8B-Instruct"  # model_id first
    assert command_list[served_model_indices[0] + 2] == "custom-aim-id"  # aim_id second
    # Verify exactly two values follow (next arg should be a different flag or end of list)
    next_index = served_model_indices[0] + 3
    assert next_index >= len(command_list) or command_list[next_index].startswith(
        "--"
    ), "Expected exactly two model names after --served-model-name"


def test_build_command_list_general_profile_with_aim_id_fallback(
    general_profiles_path: str, profile_validator: ProfileValidator, schemas_path: str
) -> None:
    """Test that general profiles can use aim_id as model fallback in model-specific containers."""
    # Create config with aim_id but no model_id (model-specific container)
    config = AIMConfig(
        aim_id="meta-llama/Llama-3.1-8B-Instruct",  # Model-specific container
        model_id=None,  # No explicit model_id
        profile_base_path=general_profiles_path,
        schema_search_path=schemas_path,
    )
    command_generator = CommandGenerator(config)

    # Load a general profile (no model_id in profile)
    registry = ProfileRegistry.discover_and_validate(search_paths=[general_profiles_path], validator=profile_validator)
    general_profile = registry.find_by_id("general/minimal_profile_no_model")

    # Should fall back to aim_id for model
    command_list = command_generator._build_command_list(general_profile)

    assert "--model" in command_list
    model_index = command_list.index("--model")
    assert command_list[model_index + 1] == "meta-llama/Llama-3.1-8B-Instruct"
