# Copyright Â© Advanced Micro Devices, Inc., or its affiliates.
#
# SPDX-License-Identifier: MIT

org:
  opencontainers:
    image:
      vendor: AMD
      authors: ''
      licenses: Apache-2.0, MIT
      description: The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative
        Sparse Mixture of Experts.
      documentation: ''
      source: https://github.com/amd-enterprise-ai/aim-build
com:
  amd:
    aim:
      model:
        canonicalName: mistralai/Mixtral-8x7B-Instruct-v0.1
        tags:
        - text-generation
        - chat
        source: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1
        variants:
        - mistralai/Mixtral-8x7B-Instruct-v0.1
        recommendedDeployments:
        - gpuModel: MI300X
          gpuCount: 1
          precision: fp16
          metric: latency
          description: Optimized for latency on MI300X using fp16 precision
        - gpuModel: MI300X
          gpuCount: 1
          precision: fp16
          metric: throughput
          description: Optimized for throughput on MI300X using fp16 precision
      hfToken:
        required: false
      release:
        notes: ''
      description:
        full: ''
